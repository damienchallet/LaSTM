{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volatility prediction with VLSTMs\n",
    "\n",
    "(c) Damien Challet and Vincent Ragel (2023)\n",
    "\n",
    "This notebook, together with the VLSTM.py file, contains the code used to produce the results of \n",
    "\n",
    "Damien Challet and Vincent Ragel, Recurrent neural networks with long and flexible memory: application to price volatility prediction (2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pdb\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM,  Activation, Reshape, Flatten, Dropout, Lambda, RepeatVector\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "\n",
    "\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "import VLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import pandas as pd\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "import scipy.special as scsp\n",
    "import statsmodels.api as sm\n",
    "import math\n",
    "import seaborn as sns; sns.set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "dirResults=\"results\"\n",
    "dirPlots=dirResults+\"/plots\"\n",
    "dirHistory=dirResults+\"/history\"\n",
    "dirModels=dirResults+\"/models\"\n",
    "\n",
    "if not os.path.isdir(dirPlots):\n",
    "    os.makedirs(dirPlots)\n",
    "\n",
    "if not os.path.isdir(dirHistory):\n",
    "    os.makedirs(dirHistory)\n",
    "\n",
    "if not os.path.isdir(dirModels):\n",
    "    os.makedirs(dirModels)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Deep Learning for Time Series Forecasting (c) Jason Brownlee\n",
    "\n",
    "#split a univariate sequence into samples\n",
    "def split_sequence_univariate(sequence, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(sequence)-1:\n",
    "            break\n",
    "            \n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "# Transforms a 2d-X and y to 3d X and y\n",
    "\n",
    "\n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences_multivariate(sequences, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "# find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "# check if we are beyond the dataset\n",
    "        if end_ix > len(sequences)-1:\n",
    "            break\n",
    "# gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source=\"Oxford\"\n",
    "Symbol=\"all\"\n",
    "\n",
    "#Oxford-Man Institute data\n",
    "DF=pd.read_csv(\"data/oxfordmanrealizedvolatilityindices.csv.gz\")\n",
    "\n",
    "DF[\"log_rk_twoscale\"]=np.log(DF[\"rk_twoscale\"])\n",
    "\n",
    "DF_ret=DF.groupby(\"Symbol\").apply(lambda df: np.log(df[\"close_price\"]).diff()).reset_index()\n",
    "DF_ret.drop(columns=\"level_1\",inplace=True)\n",
    "DF_ret.rename(columns={\"close_price\": \"ret_CC\"},inplace=True)\n",
    "\n",
    "DF[\"ret_CC\"]=DF_ret[\"ret_CC\"]\n",
    "DF.dropna(inplace=True)\n",
    "DF.rename(columns={\"Unnamed: 0\":\"date\"},inplace=True)\n",
    "if not Symbol==\"all\":\n",
    "    DF=DF[DF['Symbol']==Symbol]\n",
    "\n",
    "alldates=DF[\"date\"].unique()     # used later to define train / validation / test periods\n",
    "alldates.sort()\n",
    "\n",
    "alldates_DF=pd.DataFrame({'date':alldates,'date_idx':range(len(alldates))})     # assign one index per date\n",
    "alldates_DF.set_index(\"date\",inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "DF2=DF.groupby(\"Symbol\").apply(lambda df: df.set_index(\"date\").join(alldates_DF))\n",
    "\n",
    "DF=DF2.drop(columns=[\"Symbol\"]).reset_index()\n",
    "DF.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "DF.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences_multi_asset(data,T_seq,X_cols,Y_col):\n",
    "    \n",
    "    X_cols_eff=X_cols\n",
    "    if not Y_col in X_cols_eff:\n",
    "        X_cols_eff=X_cols_eff.append(Y_col)   # the split functions assume that Y is the last X_cols\n",
    "         \n",
    "    if(len(X_cols_eff)==1):\n",
    "        func_split=split_sequence_univariate\n",
    "    else:\n",
    "        func_split=split_sequences_multivariate\n",
    "        \n",
    "    sequs=data.groupby(\"Symbol\").apply(lambda df: func_split(df[X_cols_eff].values,T_seq))\n",
    "    \n",
    "    if not Y_col in X_cols: # one removes Y_col from predictors, then\n",
    "        print(\"TODO: not Y_col in X_cols\")\n",
    "\n",
    "    return sequs.T.to_dict()\n",
    "    \n",
    "\n",
    "import random    \n",
    "    \n",
    "def extract_sequences_from_dict(sequs_dict,t0,t1,batch_size,col_idx=0,shuffle=True):\n",
    "    \n",
    "    num_batches=(t1-t0)//batch_size\n",
    "    t0_eff=t1-num_batches*batch_size\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    \n",
    "    symbols=[*sequs_dict]\n",
    "    if shuffle and len(symbols)>1:\n",
    "        random.shuffle(symbols)\n",
    "    \n",
    "    for symbol in symbols:\n",
    "        #pdb.set_trace()\n",
    "\n",
    "        #print(sequs_symbol[0])\n",
    "        sequs_symbol=sequs_dict[symbol]\n",
    "        \n",
    "        sequs=sequs_symbol[0]\n",
    "\n",
    "        for idx in range(sequs.shape[0]):  # scans all the sequences\n",
    "            myseq=np.array(sequs[idx])\n",
    "\n",
    "            if t0<=min(myseq[:,col_idx]) and t1>max(myseq[:,col_idx]):\n",
    "                X.append(np.delete(myseq,col_idx,axis=1))\n",
    "                Y.append(sequs_symbol[1][idx])\n",
    "                 \n",
    "    return np.array(X),np.array(Y)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "num_epochs=1000\n",
    "val_split=0.2\n",
    "test_split=0.2\n",
    "\n",
    "Nruns=20\n",
    "\n",
    "forceRecompute=False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if len(DF[\"Symbol\"].unique())>1:\n",
    "    dataTag=\"Oxf_all\"\n",
    "    length_symbol=DF.groupby(\"Symbol\").apply(lambda df: df.shape[0])\n",
    "else:\n",
    "    dataTag=\"Oxf_\"+DF[\"Symbol\"].unique()\n",
    "    length_symbol=1\n",
    "\n",
    "columns_sel=[\"open_to_close\",\"ret_CC\",\"log_rk_twoscale\"]\n",
    "\n",
    "t0_train=0\n",
    "t1_train=int(length_symbol.max()*(1-val_split-test_split))\n",
    "\n",
    "t0_val=t1_train\n",
    "t1_val=int(length_symbol.max()*(1-test_split))\n",
    "\n",
    "t0_test=t1_val\n",
    "t1_test=length_symbol.max()\n",
    "\n",
    "col_idx=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params=[]\n",
    "for N_I in range(10,105,15):\n",
    "    for neurons in range(1,6):\n",
    "        for with_bias in [True,False]:\n",
    "            for model_type in [\"VLSTM\",\"LSTM\"]:\n",
    "                for bound_alpha in [True]:\n",
    "                    if model_type==\"LSTM\" and not bound_alpha:\n",
    "                        continue\n",
    "                    for i in range(Nruns):\n",
    "\n",
    "                        params.append({\"N_I\":N_I,\n",
    "                                      \"neurons\":neurons,\n",
    "                                      \"with_bias\":with_bias,\n",
    "                                      \"bound_alpha\":bound_alpha,\n",
    "                                      \"run\":i,\n",
    "                                      \"model_type\":model_type})\n",
    "                        \n",
    "print(len(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_I_prev=None\n",
    "\n",
    "for p in params:\n",
    "    N_I=p[\"N_I\"]\n",
    "    neurons=p[\"neurons\"]\n",
    "    with_bias=p[\"with_bias\"]\n",
    "    bound_alpha=p[\"bound_alpha\"]\n",
    "    run=p[\"run\"]\n",
    "    model_type=p[\"model_type\"]\n",
    "    \n",
    "    file_root=dataTag+\"_\"+columns_sel[-1]+\"_\"+model_type+\"_NH:\"+str(neurons)+\"_bias:\"+str(with_bias)\n",
    "    if model_type==\"VLSTM\":\n",
    "        file_root=file_root+\"_boundalpha:\"+str(bound_alpha)\n",
    "    file_root=file_root+\"_Tseq:\"+str(N_I)+\"_batchSize:\"+str(batch_size)+\"_predictors:\"+','.join(columns_sel)+\"_run:\"+str(run)\n",
    "    print(file_root)\n",
    "\n",
    "    if N_I!=N_I_prev:\n",
    "        sequs_dict=create_sequences_multi_asset(DF,N_I,[\"date_idx\"]+columns_sel,columns_sel[-1])\n",
    "        X_train,Y_train=extract_sequences_from_dict(sequs_dict,t0_train,t1_train,batch_size,col_idx)\n",
    "        X_val,Y_val=extract_sequences_from_dict(sequs_dict,t0_val,t1_val,batch_size,col_idx)\n",
    "        X_test,Y_test=extract_sequences_from_dict(sequs_dict,t0_test,t1_test,batch_size,col_idx)\n",
    "            \n",
    "        N_I_prev=N_I\n",
    "    \n",
    "   \n",
    "    model = Sequential()\n",
    "\n",
    "    if model_type==\"VLSTM\":\n",
    "        model.add(VLSTM.VLSTM(neurons,use_bias=with_bias,bound_alpha=bound_alpha,input_shape=(X_train.shape[1],X_train.shape[2])))\n",
    "    elif model_type==\"LSTM\":\n",
    "        model.add(LSTM(neurons,use_bias=with_bias,input_shape=(X_train.shape[1],X_train.shape[2])))\n",
    "\n",
    "    model.add(Dense(neurons,activation=\"sigmoid\"))\n",
    "    model.add(Dense(1,activation=\"linear\"))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    file_model=dirModels+\"/model_\"+file_root+\".h5\"\n",
    "    file_hist=dirHistory+\"/history_\"+file_root+\".pkl\"\n",
    "    \n",
    "    if not Path(file_hist).is_file():\n",
    "        print(\" \"+file_hist+\" does not exist, touching it\")\n",
    "        Path(file_hist).touch()\n",
    "    else:\n",
    "        if not forceRecompute:\n",
    "            print(\" \"+file_hist+\" already exists, skipping\")\n",
    "            continue\n",
    "            \n",
    "\n",
    "    # train model\n",
    "    \n",
    "    hist=model.fit(X_train,Y_train,batch_size=batch_size,epochs=num_epochs,\n",
    "                       validation_data=(X_val,Y_val),callbacks=[es],\n",
    "                       shuffle=False,verbose=1)  \n",
    "    loss_test=model.evaluate(X_test,Y_test,verbose=0)\n",
    "    hist.history[\"loss_test\"]=loss_test\n",
    "\n",
    "    with open(file_hist, 'wb') as file:\n",
    "        pickle.dump(hist.history, file)\n",
    "\n",
    "\n",
    "\n",
    "    model.save_weights(file_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.exit() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
